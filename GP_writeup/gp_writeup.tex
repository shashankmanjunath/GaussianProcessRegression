\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     % \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}        % math notation etc
\usepackage{graphicx}       % inserting images
\usepackage{float}          % image placement
\usepackage{array}          % table alignment
\usepackage{xcolor}         % to do macro

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\graphicspath{ {./figures/} }

\title{On Gaussian Processes for Regression}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  Jeffrey Alido \\
  Department of Electrical and Computer Engineering \\
  Boston University \\
  \texttt{jalido@bu.edu} \\
  \And
  Shashank Manjunath \\
  Department of Electrical and Computer Engineering \\
  Boston University \\
  \texttt{manjuns@bu.edu} \\
}

\begin{document}

\maketitle

\author

\begin{abstract}
Gaussian processes emerged in machine learning as a powerful tool for regression and classification that provides
interpretability through kernel choice and uncertainty quantification. By leveraging properties of multivariate normal
distributions and Bayesâ€™s rule, we may infer a probability distribution over possible functions when fitting a dataset.
This Bayesian framework allows flexibility through choosing a covariance function as a prior belief about the dataset,
which can provide further insight into the trends of the training data. We implement a multi-dimensional Gaussian
process regressor and evaluate its performance on the Boston Housing dataset, which is comparable to those in the top 10
of the Kaggle competition. Furthermore, we perform optimization on the hyperparameters through 5-fold cross-validation.
We provide a MATLAB implementation of Gaussian Processes for Regression at
\href{ihttps://github.com/shashankmanjunath/GaussianProcessRegression}{https://github.com/shashankmanjunath/GaussianProcessRegression}.
\end{abstract}

\section{Gaussian Random Variables}

A random variable is a function that maps from an event space to a measurable space. The event space represents a set of
all possible outcomes that the random variable may take, and the measurable space is a probability measure between 0 and
1 (inclusive). We say that a random variable $X$ is normally distributed if the event space has a Gaussian probability
distribution, fully characterized by two parameters: a mean $\mu$ and variance $\sigma^2$:

\[
  X\sim \mathcal{N}(\mu,\sigma^2)
\]

For a one-dimensional Gaussian random variable, we refer to its distribution as a univariate Gaussian distribution. A
set of Gaussian random variables may be characterized jointly as a multivariate Gaussian distribution, with joint
probability distribution fully characterized by a mean vector and a covariance matrix:

\[
  X = \begin{bmatrix}
           X_{1} \\
           X_{2} \\
           \vdots \\
           X_{n}
         \end{bmatrix}   \sim \mathcal{N}(\boldsymbol{\mu},\Sigma)
\]

where $\boldsymbol{\mu}$ is the mean vector, and $\Sigma$ is the covariance matrix whose entries describe the covariance
between each pair of random variables. Multivariate Gaussian random variables form the foundation for Gaussian
processes, by characterizing the functions drawn from the Gaussian process.

\section{Gaussian Process}

A random process is essentially a collection of random variables jointly characterized  as a set or vector of random
variables with a multivariate joint probability distribution. A Gaussian process $f(\boldsymbol{x})$ is defined as a
random process where each set of random variable in the random process is has a multivariate Gaussian distribution.
$f(\bx)$ is fully characterized by a mean function  $m(\bx)$ and covariance function,
$K(\bx,\bx')$:

\begin{gather*}
  f(\boldsymbol{x})\sim\mathcal{GP}(m(\bx),K(\bx, \bx')) \\
  K(\bx, \bx') = \E[(f(\bx) - m(\bx))(f(\bx') - m(\bx'))]
\end{gather*}

Typically, the mean function is assumed to be zero. The kernel for the covariance is chosen based on some prior belief
about the dataset; more on kernels is discussed in \ref{subsection:kernels}. In a Gaussian process, we assume that any
new observed points follow the same multivariate normal observed in our training data. GP's are non-parametric models,
unlike models such as standard linear regressors or neural networks, since they do not have "weights" in the sense that
a linear regressor or neural network has weights\cite{bishop_pattern_2006}. Rather, a GP is characterized by a mean and
covariance function, from which predicted can be drawn\cite{kuss_gaussian_2006}. While we have defined a Gaussian
process, we now aim to create a scheme under which to perform regression.

\section{Regression} \label{section:regression}

Suppose we observe training data $\bx \in \R^{n \times d}$, training labels $\by \in \R^{n}$, testing data $\bx' \in
\R^{m \times d}$ and choose kernel $\kappa$, and choose noise parameter $\sigma_{n}^{2} \in \R$. Then the mean and
covariance functions are given by 

\begin{gather*}
  m(\bx) = \kappa(\bx, \bx')^\top (\kappa(\bx, \bx) + \sigma_{n}^{2}I_{n})^{-1} \by \\
  K(\bx, \bx') = \kappa(\bx', \bx') - \kappa(\bx, \bx')^{\top} (\kappa(\bx, \bx) + \sigma_{n}^{2} I_{n})^{-1}
  \kappa(\bx, \bx')
\end{gather*}

where $I_{n}$ indicates the identity matrix. Those interested in a thorough derivation of the results are encouraged to
consult Chapter 2 of \cite{rasmussen_gaussian_2006}.

\subsection{Kernels} \label{subsection:kernels}

Covariance Functions or kernels, denoted $\kappa(\bx, \bx')$, form the core of a Gaussian process. Kernels allow
projection of input data into an alternate feature space, allowing easier separability of data in this new feature
space. Gaussian processes leverage kernels to featurize input data. In particular, if we have a function
$\Phi(\bx): \R^d \rightarrow \R^n$, we can write the kernel defined by this function as:

\[
  \kappa(\bx, \bx') = \langle \Phi(\bx), \Phi(\bx') \rangle = \Phi(\bx)^\top \Phi(\bx')
\]

We illustrate specific kernels that we used below. These kernels can additionally be combined through addition and
multiplication of the kernels together, though we do not explore this as part of this project. The kernel must be chosen
prior to fitting the GP, and is typically chosen based on the specific modelling problem at hand.

\subsubsection{Radial Basis Function (RBF) Kernel/Square Exponential Kernel}

The Radial Basis Function (RBF) Kernel, also known as the Squared Exponential Kernel, is given by:

\[
  \kappa_{RBF}(\bx, \bx') = \sigma^2 \exp\left( - \frac{\|\bx -\bx' \|_{2}^{2}}{2 \ell^2} \right)
\]

This kernel is parametrized by two parameters, the lengthscale $\ell$ and the variance $\sigma^2$. The lengthscale
determines the width of the kernel, and the variance scales the kernel\cite{duvenaud_automatic_2014}. We provide an
images of the RBF Kernel with various lengthscales and variances in Figure~\ref{fig:square_exp_kernel}

\begin{figure}[H]
  \centering
  \caption{Graph of a square exponential kernel/RBF kernel for various lengthscales and variances}
  \label{fig:square_exp_kernel}
  \includegraphics[trim={0 7cm 0 7cm},clip,width=\textwidth]{square_exp_kernel}
\end{figure}

\subsubsection{Rational Quadratic Kernel}

The Rational Quadratic Kernel is another standard kernel is similar to the RBF kernel. It can be constructed from
summing RBF kernels with varying lengthscales. The kernel is given by:

\[
  \kappa_{RQ}(\bx, \bx') = \sigma^2 \left( 1 + \frac{\| \bx - \bx' \|_{2}^{2}}{2 \alpha \ell^2} \right)^{-\alpha}
\]

This kernel is parametrized by three parameters, the lengthscale $\ell$, the variance $\sigma^2$, and the lengthscale
weighting parameter $\alpha$\cite{duvenaud_automatic_2014}. We provide images of the Rational Quadratic Kernel with
various $\alpha$ values in Figure~\ref{fig:rqk}

\pagebreak

\begin{figure}[H]
  \centering
  \caption{Graph of a rational quadratic kernel for $\sigma = 1$, $\ell = 1$, and various $\alpha$ values}
  \label{fig:rqk}
  \includegraphics[trim={0 7.5cm 0 7cm},clip,scale=0.6]{rqk}
\end{figure}

\subsubsection{Periodic Kernel}

The periodic kernel allows us to model periodic functions. It is given by: 

\[
  \kappa_{P}(\bx, \bx') = \sigma^{2} \exp\left(- \frac{2 \sin^{2}(\pi \|\bx - \bx'\|)}{p \ell^{2}}\right)
\]

This kernel parametrized by two parameters, $p$ which describes the period of the function, and $\ell$ which is the
lengthscale\cite{duvenaud_automatic_2014}.  We provide images of the Periodic Kernel with various $p$ values in
Figure~\ref{fig:periodic_kernel}.

\begin{figure}[H]
  \centering
  \caption{Graph of a rational quadratic kernel for $\sigma = 1$, $\ell = 1$, and various $p$ values}
  \label{fig:periodic_kernel}
  \includegraphics[trim={0 7.5cm 0 7cm},clip,keepaspectratio=true,scale=0.6]{periodic_kernel}
\end{figure}

\subsubsection{Other Kernels}

We additionally test other kernels, including the Linear Kernel, Local Periodic Kernel, and the Polynomial Kernel. The
Linear kernel has no hyperparameters, and is given by:

\[
  \kappa(\bx, \bx') = \langle \bx, \bx' \rangle
\]

The polynomial kernel has hyperparameters $c$, which controls the zero crossing of the kernel, and $d$ which controls the
order of the polynomial. This kernel is given by:

\[
  \kappa(\bx, \bx') = (\bx^{\top}\bx' + )^{d}
\]

We additionally test one kernel which is a combination of two kernels already discusses, the Local Periodic Kernel.
This kernel is the product of a periodic kernel and a square exponential kernel, and is given by:

\[
  \kappa(\bx, \bx') = \sigma^2 \exp\left( - \frac{\|\bx -\bx' \|_{2}^{2}}{2 \ell^2} \right)  \exp(-
  \frac{2 \sin^{2}(\pi \|\bx - \bx'\|)}{p \ell_{p}^{2}})
\]

This kernel is parametrized by $\sigma^{2}$, the overall variance, $\ell_p$, the lengthscale of the periodic function,
$p$, the period of the periodic function, and $\ell$, the lengthscale of the square exponential kernel.


\section{A Simple Demo}
Here we demonstrate GP regression on sinusoidal data fit using a square exponential kernel with lengthscale $\ell =
1.5$, and variance $\sigma^{2} = 1.0$. The true function is represented by the black dashed line, with training points
represented by the solid red dots. The orange yellow and blue curve represent samples from the distribution of possible
functions, and the red dashed line represents the 95\% confidence interval.

\begin{figure}[H]
  \centering
  \label{fig:synthetic_data}
  \includegraphics[trim={0cm, 4cm, 0cm, 3cm},clip,scale=0.5]{synthetic_data}
  \caption{Visualization of a Gaussian process. The dashed black line is the synthetic function (a sinusoid), not
  provided to the GP. We provide the red training points, and fit the GP as described in Section
\ref{section:regression}. We then draw three example functions (blue, red, and yellow solid lines) from the GP, as well
as calculate the bounds for the 95\% confidence intervals (dashed red lines)}
\end{figure}

\section{Boston Housing Dataset}

The Boston Housing Dataset, originally published in 1978 contains 506 data points, each containing 13 features and 1
label for regression\cite{harrison_hedonic_1978}. The dataset provides the median value of houses in Boston suburbs.
This dataset is particularly suitable for Gaussian processes, as the dataset is quite small. Use of Gaussian processes
allows us to accurately quantify our uncertainty for each The label is the median value of owner-occupied homes in \$1000s, and
all other features are used for model fitting. The features included in the dataset are given in the table in Table
\ref{table:bhd_feat}.

\begin{table}[H]
  \centering
  \caption{Table of Boston Housing Dataset feature names and features}
  \begin{tabular}{ || m{3cm} | m{7cm} || }
    \hline
    \textbf{Feature Name} & \textbf{Feature Description} \\
    \hline \hline
    CRIM    & Per capita crime rate by town \\
    \hline
    ZN      & Proportion of residential land zoned for lots over 25,000 sq.ft. \\
    \hline
    INDUS   & Proportion of non-retail business acres per town. \\
    \hline
    CHAS    & Charles River dummy variable (1 if tract bounds river; 0 otherwise) \\
    \hline
    NOX     & Nitric oxides concentration (parts per 10 million) \\
    \hline
    RM      & Average number of rooms per dwelling \\
    \hline
    AGE     & Proportion of owner-occupied units built prior to 1940 \\
    \hline
    DIS     & Weighted distances to five Boston employment centres \\
    \hline
    RAD     & Index of accessibility to radial highways \\
    \hline
    TAX     & Full-value property-tax rate per \$10,000 \\
    \hline
    PTRATIO & Pupil-teacher ratio by town \\
    \hline
    B       & $1000(Bk - 0.63)^2$ where Bk is the proportion of Black people by town \\
    \hline
    LSTAT   & \% lower status of the population \\
    \hline
    MEDV    & Median value of owner-occupied homes in \$1000's \\
    \hline
  \end{tabular}
  \label{table:bhd_feat}
\end{table}

For the application of Gaussian Processes, we use the regression task, i.e. fitting to the MEDV feature. Prior to
fitting on the data, we normalize the data per feature. Specifically, for each feature in the dataset, we perform the
following operation:

\[
  X_{\text{feat}} = \frac{X_{\text{feat}} - \mu(X_{\text{feat}})}{\sigma(X_{\text{feat}})}
\]

where $\mu(X)$ is the mean value of that feature in the training set, and $\sigma(X)$ is the standard deviation of that
feature in the training set. We additionally normalize the MEDV feature, and convert it back to non-normalized units
before calculating our RMSE.

\section{Results}

We perform regression on the Boston Housing Dataset using our own GP implementation formulated as described in Section
\ref{section:regression}, MATLAB's built-in SVM Regressor function, and our own implementation of linear regression.
5-fold cross validation hyperparameter search was performed in training and the root mean squared error (RMSE) is used
as our performance metric. More specifically, we calculate the RMSE using the following formula:

\[
  RMSE = \sqrt{\frac{\sum\limits_{i=1}^{N} (y_i - \hat y_i)^{2}}{N}}
\]

where $y_i$ is the true value, $\hat y_i$ is the predicted mean value of our GP, and $N$ is the number of samples. Our
results are shown in Table 2.

The smallest RMSE is the GP with a rational quadratic kernel, indicating the best performance. The rational quadratic
kernel is equivalent to the sum of RBF kernels with different lengthscales, for increased robustness. The poor
performance of periodic and locally periodic kernels reveal that there is likely no periodic trends in the data.

The results from the GP regression provide further information than SVM or linear regression through uncertainty
quantification. This feature, along with incorporating prior beliefs through kernel choice gives the GP algorithm its
interpretability.

\begin{table}[H]\label{table:results}
\caption{Table of Boston Housing Dataset feature names and features}
\resizebox{.7\textheight}{!}{
  {
  \centering
  \begin{tabular}{ || m{3cm} | m{3cm} | m{3cm} | m{3cm} || }
    \hline
    & \textbf{Gaussian Process Regressor} & \textbf{SVM Regressor} & \textbf{Linear Regression} \\
    \hline
    Linear Kernel & \textbf{4.751} & 4.935 & \textbf{4.751} \\
    \hline
    Square Exp Kernel & \textbf{3.586} & 9.014 & 4.751 \\
    \hline
    Rational Quadratic Kernel & \textbf{3.560} & 9.013 & 4.751 \\
    \hline
    Periodic Kernel & 8.944 & 9.017 & \textbf{4.751} \\
    \hline
    Local Periodic Kernel & 5.065 & 9.014 & \textbf{4.751} \\
    \hline
    Polynomial Kernel & \textbf{4.213} & 80.389 & 4.751 \\
    \hline
  \end{tabular}
  }
}
\end{table}

\section{Conclusions}

In this writeup, we provided a brief introduction to Gaussian Processes, discussed algorithms to fit GPs for regression,
provided a simple demo, and tested out GPs with various kernels on the Boston Housing dataset, showing good algorithm
performance. We additionally provide our implementation in MATLAB at
\href{https://github.com/shashankmanjunath/GaussianProcessRegression}{https://github.com/shashankmanjunath/GaussianProcessRegression}.

\bibliographystyle{plain}
\bibliography{gpr}

\end{document}
